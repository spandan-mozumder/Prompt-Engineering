{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load OpenAI's ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_client = OpenAI(\n",
    "    api_key=\"Your Github API Key\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo Message Passing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = [{\"role\": \"user\", \"content\": \"What is the capital of France?\"}]\n",
    "\n",
    "output = openai_client.chat.completions.create(\n",
    "    model = \"gpt-4o-mini\",\n",
    "    messages = message\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.choices[0].message[0].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = [{\"role\": \"user\", \"content\": \"Give me 3 names of new AI assistants?\"}]\n",
    "\n",
    "output = openai_client.chat.completions.create(\n",
    "    model = \"gpt-4o-mini\",\n",
    "    messages = message,\n",
    "    temperature = 0.7\n",
    ")\n",
    "\n",
    "output.choices[0].message[0].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI's Response Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_openai(open_client, prompt, temperature = 0.5):\n",
    "    message = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    output = openai_client.chat.completions.create(\n",
    "        model = \"gpt-4o-mini\",\n",
    "        messages = message,\n",
    "        temperature = temperature\n",
    "    )\n",
    "    return output.choices[0].message[0].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For handling multiple messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_openai(open_client, message, temperature = 0.5):\n",
    "    output = openai_client.chat.completions.create(\n",
    "        model = \"gpt-4o-mini\",\n",
    "        messages = message,\n",
    "        temperature = temperature\n",
    "    )\n",
    "    return output.choices[0].message[0].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is the capital of France?\"\n",
    "\n",
    "output = get_response_openai(openai_client, prompt)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load LLama from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Your HuggingFace Model Name\"\n",
    "\n",
    "hf_token = \"Your HuggingFace Token\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_hub.login(hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.random.manual_seed(42)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map = device,\n",
    "    torch_dtype = \"auto\",\n",
    "    trust_remote_code = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_hf(pipe, prompt, temperature=0.5):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    sampling = False if temperature == 0 else True\n",
    "\n",
    "    generation_args = {\n",
    "        \"max_new_tokens\": 2000,\n",
    "        \"return_full_text\": False,\n",
    "        \"temperature\": temperature,\n",
    "        \"do_sample\": sampling\n",
    "    }\n",
    "\n",
    "    ouput = pipe(messages, **generation_args)\n",
    "    return ouput[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For handling multiple messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_from_messages_hf(pipe, messages, temperature=0.5):\n",
    "    sampling = False if temperature == 0 else True\n",
    "\n",
    "    generation_args = {\n",
    "        \"max_new_tokens\": 2000,\n",
    "        \"return_full_text\": False,\n",
    "        \"temperature\": temperature,\n",
    "        \"do_sample\": sampling\n",
    "    }\n",
    "\n",
    "    ouput = pipe(messages, **generation_args)\n",
    "    return ouput[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principle 1 - Write clear and specific instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Delimiters to clearly indicate parts of the prompt (xml tag, ticks, dashes, backticks, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = \"This product is Awesome and I like it so much\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Determine the sentiment (Positive, Negative, Neutral) of the following review.\n",
    "The review is between three backticks\n",
    "\n",
    "```{review}```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = get_response_openai(openai_client, prompt)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = get_response_hf(pipe, prompt)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ask for a structured output (json, xml, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = \"This product is Awesome and I like it so much\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Determine the sentiment (Positive, Negative, Neutral) of the following review.\n",
    "The review is between three backticks\n",
    "\n",
    "```{review}```\n",
    "\n",
    "Generate the answer in a JSON format that has the following fields:\n",
    "-\"sentiment\" - string that is one of the following values: \"Positive\", \"Negative\", \"Neutral\"\n",
    "\n",
    "Always respond with a valid JSON. Do Not include any extra characters, symbols or text outside the JSON itself\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = get_response_openai(openai_client, prompt)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = get_response_hf(pipe, prompt)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json.loads(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few Shot Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = \"The lighting in the shop is so warm, and it makes the place feel inviting\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Determine the category of the review\n",
    "It's one of those 4 options: \"Service\", \"Quality\", \"Ambience\", or \"Pricing\n",
    "The user review is between three backticks\n",
    "\n",
    "```{review}```\n",
    "\n",
    "Here are some examples of reviews and their categories:\n",
    "\n",
    "Review: \"The barista was incredibly friendly and made my drink quickly.\"\n",
    "Category: \"Service\"\n",
    "\n",
    "Review: \"The cappuccino was perfect, and ther beans tasted fresh.\"\n",
    "Category: \"Quality\"\n",
    "\n",
    "Review: \"I love the cozy seating and relaxing music in the shop\" \n",
    "Category: \"Ambience\"\n",
    "\n",
    "Review: \"the prices are a bit too high compared to other cafes nearby\" \n",
    "Category: \"Pricing\"\n",
    "\n",
    "\n",
    "Generate the answer in a JSON format that has the following fields:\n",
    "-\"sentiment\" - string that is one of the following values: \"Service\", \"Quality\", \"Ambience\", \"Pricing\"\n",
    "\n",
    "Always respond with a valid JSON. Do Not include any extra characters, symbols or text outside the JSON itself\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = get_response_hf(pipe, prompt)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principle 2 - Give the model time to think"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the model think step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "I bought two balls with 10$. One ball is more expensive tha the other by 1$.\n",
    "How much is the expensive ball?\n",
    "\n",
    "Please provide the answer in one number\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = get_response_hf(pipe, prompt)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "I bought two balls with 10$. One ball is more expensive tha the other by 1$.\n",
    "How much is the expensive ball?\n",
    "\n",
    "Please think about this step by step and at the end please provide the answer\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = get_response_hf(pipe, prompt)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the steps required to complete task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "How many r's are in Strawberry?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = get_response_hf(pipe, prompt)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "How many r's are in Strawberry?\n",
    "\n",
    "Follow the steps below to count the number of r's in Strawberry\n",
    "1. Break down the word into letters\n",
    "2. For each letter write 1 if its and r and 0 if it isn't\n",
    "3. Now have a counter that counts the number of 1's\n",
    "4. Now write down the final answer\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = get_response_hf(pipe, prompt)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_review = \"\"\"\n",
    "I recently visited Coffe Haven and ordered a caramel latte with almond milk. \\\n",
    "The staff was friendly, but the service was slow. It took almost 20 minutes to get my drink. \\\n",
    "The latte itself was too sweet for my liking, and I could barely taste the coffee. \\\n",
    " However, the ambience was cozy, and I loved the music they played. \\\n",
    " I might come back for the coffee, but not for the coffee.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "The user review is between xml tag called customer_review\n",
    "\n",
    "<customer_review>\n",
    "{customer_review}\n",
    "</customer_review>\n",
    "\n",
    "Please summarize this review in one sentence\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = get_response_hf(pipe, prompt)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "The user review is between xml tag called customer_review\n",
    "\n",
    "<customer_review>\n",
    "{customer_review}\n",
    "</customer_review>\n",
    "\n",
    "Please translate the customer review into French and provide only the translated text with no xml tags\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = get_response_hf(pipe, prompt)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract Named Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "The user review is between xml tag called customer_review\n",
    "\n",
    "<customer_review>\n",
    "{customer_review}\n",
    "</customer_review>\n",
    "\n",
    "Use only the list of named entities below:\n",
    "Person Names\n",
    "Organizations\n",
    "Locations\n",
    "Cities\n",
    "Countries\n",
    "Continents\n",
    "Regions\n",
    "Dates\n",
    "Times\n",
    "Monetary Values\n",
    "Percentages\n",
    "Quantities\n",
    "Products\n",
    "Events\n",
    "\n",
    "Please extract all named entities and their types. \n",
    "Use the following format.\n",
    "named entity: type\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = get_response_hf(pipe, prompt)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "The user review is between xml tag called customer_review\n",
    "\n",
    "<customer_review>\n",
    "{customer_review}\n",
    "</customer_review>\n",
    "\n",
    "Analyze the text corpus above and extract the key topics discussed:\n",
    "\n",
    "For each topic:\n",
    "\n",
    "Provide a clear and concise topic title.\n",
    "List the top 5 most representative keywords for that topic.\n",
    "Group related sentiments or phrases from the corpus under the identified topic\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = get_response_hf(pipe, prompt)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "The user review is between three backticks\n",
    "\n",
    "```{customer_review}```\n",
    "\n",
    "Generate the answer in a JSON format that has the following fields:\n",
    "- \"products\" - string name of product\n",
    "- \"sentiment\" - string that is one of those values \"Positive\", \"Negative\", \"Neutral\"\n",
    "- \"main likes\" - string with the user's main likes with the product\n",
    "- \"main dislikes\" - string with the user's main dislikes with the product\n",
    "\n",
    "Always respond with valid JSON. Do not include any extra characters, symbols or text outside the JSON itself\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = get_response_hf(pipe, prompt)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = \"This product is Awesome and I like it so much\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Determine the sentiment (Positive, Negative, Neutral) of the following review.\n",
    "The review is between three backticks\n",
    "\n",
    "```{review}```\n",
    "\n",
    "Generate the answer in a JSON format that has the following fields:\n",
    "-\"sentiment\" - string that is one of the following values: \"Positive\", \"Negative\", \"Neutral\"\n",
    "\n",
    "Always respond with a valid JSON. Do Not include any extra characters, symbols or text outside the JSON itself\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = get_response_hf(pipe, prompt)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "email = \"\"\" \n",
    "Dear Beloved,\n",
    "I am Prince Okoro of Nigeria, and I need your urgent elp to transfer $15 million USD into your account. \\\n",
    "In return you will receive 30% of the total amount. Please reply with your banking details so we can proceed immediately. This is a time sensitive matter.\n",
    "Best Regards,\n",
    "Prince Okoro\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "The email is between xml tag called email. You're aim is to detect whether it's spam or not.\n",
    "\n",
    "<email>\n",
    "{email}\n",
    "</email>\n",
    "\n",
    "Genrate the answer in a JSON format that has the following field:\n",
    "- \"spam\" - string that is one of those values \"Spam\", \"Not Spam\"\n",
    "\n",
    "Always respond with a valid JSON. Do not include any extra characters, symbols or text outside the JSON itself\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = get_response_hf(pipe, prompt)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a Chatbot for a specific context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "System Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = f\"\"\"\n",
    "You are a sentiment classifier that classifies reviews into three categories Positive, Negative, Neutral.\n",
    "\n",
    "You generate the answer in a JSON format that has the following field:\n",
    "- \"sentiment\" - string that is one of the following values: \"Positive\", \"Negative\", \"Neutral\"\n",
    "\n",
    "Always respond with a valid JSON. Do not include any extra characters, symbols or text outside the JSON itself\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_comment = \"This product is Awesome and I like it so much\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_comment}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = get_response_from_messages_hf(pipe, messages)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversational Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is the fastest animal in the world?\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = get_response_from_messages_hf(pipe, messages)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append({\"role\": \"assistant\", \"content\": output })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append({\"role\": \"user\", \"content\": \"What is its average size?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = get_response_from_messages_hf(pipe, messages)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customer Service Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are OrderBot, an automated service to collect orders for a sandwich shop.\n",
    "You first greet the customer, then collect the order,\n",
    "and then ask if it's a pickup or delivery.\n",
    "You wait to collect the entire order, then summarize it and check for a final\n",
    "time if the customer wants to add anything else.\n",
    "If it's a delivery, you ask for an address.\n",
    "Finally, you collect the payment.\n",
    "Make sure to clarify all options, extras, and sizes to uniquely\n",
    "identify the item from the menu.\n",
    "You respond in a short, very conversational friendly style.\n",
    "\n",
    "The menu includes:\n",
    "\n",
    "Sandwiches:\n",
    "\n",
    "Turkey Sandwich: Large $12.50, Medium $9.75, Small $7.00\n",
    "Ham and Cheese Sandwich: Large $11.95, Medium $9.25, Small $6.50\n",
    "Veggie Sandwich: Large $10.95, Medium $8.75, Small $6.00\n",
    "BLT Sandwich: Large $13.50, Medium $10.50, Small $7.50\n",
    "\n",
    "Sides:\n",
    "\n",
    "French Fries: Large $5.00, Medium $4.00, Small $3.00\n",
    "Onion Rings: Large $6.50, Medium $5.25, Small $4.00\n",
    "Garden Salad: $7.50\n",
    "\n",
    "Toppings:\n",
    "\n",
    "Extra Cheese $2.00\n",
    "Avocado $2.50\n",
    "Bacon $3.00\n",
    "Pickles $1.50\n",
    "Jalapeños $1.25\n",
    "\n",
    "Drinks:\n",
    "\n",
    "Coke: Large $3.00, Medium $2.50, Small $1.75\n",
    "Sprite: Large $3.00, Medium $2.50, Small $1.75\n",
    "Bottled Water: $5.00\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = [{\"role\": \"system\", \"content\": system_prompt}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = get_response_from_messages_hf(pipe, context)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.append({\"role\": \"assistant\", \"content\": output})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.append({\"role\": \"user\", \"content\": \"I'd like to have a Turkey sandwich\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = get_response_from_messages_hf(pipe, context)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.append({\"role\": \"assistant\", \"content\": output})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.append({\"role\": \"user\", \"content\": \"I'd like it small please\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = get_response_from_messages_hf(pipe, context)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.append({\"role\": \"assistant\", \"content\": output})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in context:\n",
    "  print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_messages(_):\n",
    "    prompt = inp.value_input\n",
    "    inp.value = ''\n",
    "\n",
    "    context.append({'role': 'user', 'content': f\"{prompt}\"})\n",
    "\n",
    "    response = get_response_from_messages_hf(pipe, context)\n",
    "    response = response\n",
    "\n",
    "    context.append({'role': 'assistant', 'content': f\"{response}\"})\n",
    "\n",
    "    panels.append(\n",
    "        pn.Row('User:', pn.pane.Markdown(prompt, width=600)))\n",
    "    panels.append(\n",
    "        pn.Row('Assistant:', pn.pane.Markdown(response, width=600 )))\n",
    "\n",
    "    return pn.Column(*panels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import panel as pn\n",
    "pn.extension()\n",
    "\n",
    "panels = []\n",
    "\n",
    "context = [{'role': 'system', 'content': system_prompt} ]\n",
    "\n",
    "\n",
    "inp = pn.widgets.TextInput(value = \"Hi\", placeholder = 'Enter text here…')\n",
    "button_conversation = pn.widgets.Button(name = \"Chat!\")\n",
    "\n",
    "interactive_conversation = pn.bind(collect_messages, button_conversation)\n",
    "\n",
    "dashboard = pn.Column(\n",
    "    inp,\n",
    "    pn.Row(button_conversation),\n",
    "    pn.panel(interactive_conversation, loading_indicator = True, height = 300),\n",
    ")\n",
    "\n",
    "dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_corpus = []\n",
    "text_corpus.append(\"\"\"\n",
    "The red panda, often called the \"firefox,\" is a small mammal native to the Himalayan region and parts of China. Despite its name, the red panda is not closely related to the giant panda but instead shares similarities with raccoons. With its vibrant reddish-brown fur, bushy tail marked with rings, and adorable mask-like facial markings, the red panda is a master of camouflage in its forested habitat. It primarily feeds on bamboo but also consumes fruits, berries, and insects. Unfortunately, this fascinating creature is endangered due to habitat loss and poaching, making conservation efforts crucial to its survival.\n",
    "\"\"\")\n",
    "\n",
    "text_corpus.append(\"\"\"\n",
    "The iPhone 15 Pro is Apple's latest flagship, pushing the boundaries of smartphone technology. With its aerospace-grade titanium body, it’s lighter yet more durable than its predecessors. The device features the A17 Pro chip, delivering lightning-fast performance for apps, gaming, and multitasking. The 48-megapixel main camera offers advanced computational photography, making it easier to capture stunning images even in challenging lighting conditions. The iPhone 15 Pro also introduces USB-C connectivity for faster data transfer and universal compatibility. This sleek smartphone combines elegance, power, and innovation in one seamless package.\n",
    "\"\"\")\n",
    "\n",
    "text_corpus.append(\"\"\"\n",
    "The dolphin is a highly intelligent marine mammal known for its playful nature and friendly interactions with humans. Dolphins live in oceans and rivers worldwide, and they are famous for their sleek, streamlined bodies and curved dorsal fins. They communicate using clicks, whistles, and body movements and are often seen leaping out of the water or riding waves. Dolphins mainly eat fish and squid, using their sharp teeth to catch prey. These social animals live in groups called pods, working together to hunt and protect one another. Loved by many, dolphins symbolize joy and freedom.\n",
    "\"\"\")\n",
    "\n",
    "text_corpus.append(\"\"\"\n",
    "The Samsung Galaxy S23 Ultra is a powerhouse designed for those who demand the best in smartphone performance. Featuring a stunning 6.8-inch Dynamic AMOLED 2X display with a 120Hz refresh rate, it delivers a buttery-smooth viewing experience. The device is powered by the Snapdragon 8 Gen 2 chipset, ensuring top-notch performance for gaming and productivity. Its standout feature is the 200-megapixel primary camera, capable of capturing incredible detail and vibrant colors in photos. With its integrated S Pen for note-taking and creative tasks, the Galaxy S23 Ultra is more than just a phone—it’s a versatile tool for work and play.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open AI Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai_client.embeddings.create(\n",
    "    input = text_corpus[0],\n",
    "    model = \"text-embedding-3-small\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(response.data[0].embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_text_openai(openai_client,text):\n",
    "  response = openai_client.embeddings.create(\n",
    "    input = text,\n",
    "    model = \"text-embedding-3-small\"\n",
    "  )\n",
    "  return response.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "\n",
    "for text in text_corpus:\n",
    "  embeddings.append(embed_text_openai(openai_client, text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HuggingFace Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.output_hidden_states = True\n",
    "def embed_text_hf(text, tokenizer, model, device = device):\n",
    "    inputs = tokenizer(text, return_tensors = \"pt\", truncation = True, padding = True)\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}  # Move inputs to the same device as the model\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(**inputs)\n",
    "\n",
    "    last_hidden_state = output.hidden_states[-1]\n",
    "    embeddings = torch.mean(last_hidden_state, dim = 1).squeeze()\n",
    "    embeddings = embeddings.to('cpu').tolist()\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "for text in text_corpus:\n",
    "  embeddings.append(embed_text_hf(text,tokenizer, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(embeddings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters = n_clusters, random_state = 42)\n",
    "kmeans.fit(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "for i, label in enumerate(cluster_labels):\n",
    "    print(f\"{text_corpus[i][:20]} :: belongs to cluster {label}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
